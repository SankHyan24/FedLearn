# FedPer

## ——带个性化层的联邦学习更新机制

与FedAvg相比，FedPer将模型分成了基础层（Base Layers）和个性化层（Personal Layers）

每次进行更新的时候，每个client将模型在本地进行更新，但是只将基础层上传到server进行与FedAvg算法相同的平均过程，个性化层保留。也就是说，服务器只掌握模型的基础层。

图示中蓝色部分为基础层，其他颜色部分为个性化层，对应不同本地数据可能出现的统计性上的分布不均。

![image-20220907205344760](https://cdn.jsdelivr.net/gh/SankHyan24/image1/img/202209072053026.png)

由于每个客户端实际上都有针对于本地数据训练出的个性化参数，这种算法可以相对有效地对抗由于noniid的数据分布对模型训练的影响。

## 算法：

客户端更新算法：

<img src="https://cdn.jsdelivr.net/gh/SankHyan24/image1/img/202209072056350.png" alt="image-20220907205611283" style="zoom:80%;" />

服务器端更新算法：

<img src="https://cdn.jsdelivr.net/gh/SankHyan24/image1/img/202209072056764.png" alt="image-20220907205640704" style="zoom: 80%;" />

## 其他：

（我从论文里复制翻译的

### 基础层是否能学到东西？

理论上，个性化层可能足以完成手头的学习任务，从而使基础层变得多余。为了测试是否是这种情况，我们将每个客户端的基础层替换为线性全连接层，并使用 FedPer 进行训练。如果基础层是冗余的，那么在训练此模型时，测试准确度（与 ResNet-34 或 MobileNet-v1 相比）不应有显着下降。图 5 和图 6 证实情况并非如此，至少 CIF AR-100 是这样。事实上，纯本地训练的性能证实了每个客户端的训练样本数量不足以以高准确度学习单个客户端模型。
